## 251122 ##

### Agent GRN correction

- **Research & Reading**
  - [Reviewed *Recursive Language Models* article (Medium)](https://alexzhang13.github.io/blog/2025/rlm/)
  - Reâ€‘read the Wikipedia paper on ranking methodology (clarify if ranking is based on confidence or textâ€‘support matching)
  - Studied Google Research â€œconfidenceâ€ paper
  - Examined KGValidator framework (arXivâ€¯2404.15923) and evaluated its scope for future work. çœ‹èµ·ä¾†Lunä»–æ˜¯éé ‚æœƒæˆ–æ˜¯é ‚åˆŠçš„paperå°±ä¸çœ‹ï¼Œæ‰€ä»¥é€™ç¯‡ä¸åœ¨ä»–çš„è€ƒæ…®ç¯„åœã€‚
  - Investigated edgeâ€‘validation cases (e.g., BCL2L1â€‘BAD, BMI1â€‘H2AX, PRKACAâ€‘AKT2, ALOX5â€‘STAT5A) and differences in edge definitions (direct vs. indirect)
    - ä¹Ÿè¨±false edgesè¡¨ç¾æ¯”è¼ƒå¥½çš„ç†ç”±å¯èƒ½æ˜¯å› ç‚ºæˆ‘çš„edgeå•æ³•å¤ªstrictäº†å—ï¼Œæ‰€ä»¥æ¯”è¼ƒå®¹æ˜“å›ç­”false
    - å¦‚æœæˆ‘çµ¦æ­£ç¢ºçš„referenceçµ¦claudeï¼Œç„¶å¾Œå•ä»–ä¸€æ¨£çš„å•é¡Œï¼Œä»–æœƒå›ç­”ä»€éº¼ -> ç­”æ¡ˆé‚„æ˜¯ä¸è®Š
    - ç‚ºä»€éº¼GLM-4.6åœ¨false edgeçš„è¡¨ç¾æ¯”è¼ƒå·®ï¼Ÿ
        - çœ‹ALOX5 - STAT5A -> çœ‹èµ·ä¾†æ˜¯å°é€£æ¥çš„å®šç¾©ä¸åŒï¼Œæœ‰äº›æ˜¯çœ‹ç›´æ¥é€£æ¥ï¼Œæœ‰äº›æ˜¯é–“æ¥å°±å¯ä»¥
        - ç”¨ç”¨çœ‹claudeçš„searchï¼Œçœ‹ä»–æœƒciteå“ªç¯‡ï¼Œçµ¦å‡ºä»€éº¼ç­”æ¡ˆï¼Œé€™å€‹edgeæ˜¯signoråœ¨2020å¹´åŠ å…¥çš„edgeï¼Œå¾Œä¾†åˆªæ‰äº†-> çµæœsignoråè€Œæ˜¯ç”¨2007å¹´çš„paper

- **Presentation Preparation**
  - Loaded the PowerPoint deck and searched Google Drive for additional 1â€‘onâ€‘1 PPT examples (found via â€œ1â€‘onâ€‘1â€ query)
  - Determined presentation length: 25â€¯min total â†’ allocate ~10â€¯min for discussion, target 15â€“20 slides

- **Planning**
  - Drafted a 3â€“4â€‘month roadmap, aiming for one major deliverable per month
  - Outlined next steps for each month (e.g., validation framework, largeâ€‘scale simulation, final report)

- **Experimental & Validation Work**
  - Set up codebase capable of running largeâ€‘scale simulations
  - Investigated why certain LLM queries are flagged as problematic
  - Verified â€œtrue edgeâ€ cases and analyzed falseâ€‘edge performance (possible overâ€‘strict query formulation)
  - Compared groundâ€‘truth tables for AU dataset
  - Tested Claude with correct references to see answer consistency
  - Analyzed GLMâ€‘4.6â€™s poorer performance on false edges
  - Ran Claudeâ€™s webâ€‘search mode on edges that all models answered â€œtrueâ€ (e.g., BCL2L1â€‘BAD â†’ Signor 2000 paper)
  - Checked citation behavior: Claude cited a 2007 paper for a 2020 Signor edge that was later removed

- **Miscellaneous Tasks**
  - Purchased Gemini-3 via VPN, using TWD payment method. I tried the å­¸ç”Ÿæ–¹æ¡ˆï¼Œä½†æˆ‘ä¸èƒ½ç”¨
  - Updated PPT color scheme according to Google Research guidelines
  - Noted potential future work on KGValidator (small scope, consider alternatives)

## Thinking:
ç¾åœ¨æˆ‘æƒ³æ¯”è¼ƒåˆç†çš„æµç¨‹å¯èƒ½æ˜¯query -> web search -> paper collections -> score the papers -> include papers to the structured query -> LLM reasoning for both Yes/No answer -> final answer -> non-reasoning LLM to give confidence -> calculate accuracy -> back to optimize the structured query. æˆ‘ä¸å¤ªç¢ºå®šçš„æ˜¯åœ¨é€™ç‹€æ³ä¸‹LLM confidenceæœ‰ä»€éº¼æ„ç¾©ï¼Œé‚„æ˜¯web searchçš„paper scoringè¦è·Ÿconfidenceçµåˆç®—å‡ºä»€éº¼æœ‰ç”¨çš„metricï¼Œç„¶å¾Œæˆ‘å€‘åœ¨é‡å°é€™å€‹åˆ†æ•¸è·ŸaccuracyåšæŸç¨®æœ€ä½³åŒ–ï¼Œä¹Ÿè¨±æˆ‘è©²å¾äººé¡çš„è§’åº¦ä¾†æƒ³é€™å€‹å•é¡Œï¼Œexpertæ˜¯æ€éº¼curate edgeçš„ï¼Œé¦–å…ˆä»–å€‘æœƒå»googleæ‰¾åˆ°çš„paperï¼Œç„¶å¾Œè©•ä¼°é€™ç¯‡paperçš„å¯é æ€§ï¼Œå¯é æ€§æœ‰å¯èƒ½åƒè€ƒabstract, images, impact factor, citationç­‰ç­‰ï¼Œæ¥è‘—å†å»çœ‹å…¶ä»–ç¯‡paperåšä¸€æ¨£çš„äº‹ï¼Œæœ‰äº†é€™äº›paperçš„ç¶œåˆè€ƒæ…®å¾Œï¼Œä»–å€‘æ‡‰è©²æœƒä¸»è¦çœ‹æœ€å¯é çš„é‚£ç¯‡paperæ˜¯supporté‚„æ˜¯ä¸supporté€™å€‹å‡èªªï¼Œå¦‚æœæœ‰äº›caseæœ‰å…©ç¯‡ç›¸åŒå¯é æ€§ï¼Œä¸”supportä¸åŒstatementçš„è©±ï¼Œé‚£éº¼å°ˆå®¶å°±æœƒçµ¦å‡ºä¸€å€‹è·Ÿè‡ªå·±çš„believeæ¯”è¼ƒæ¥è¿‘çš„ç­”æ¡ˆï¼Œåœ¨é€™ç¨®ç‹€æ³ä¸‹confidenceåæ‡‰çš„æ˜¯ä»€éº¼å‘¢ï¼Œå¦‚æœæ˜¯æœ€å¾Œå›ç­”çš„confidenceï¼Œé‚£éº¼é€™å…¶å¯¦æ˜¯åœ¨è©•ä¼°æ­£åå…©æ–¹supporting dataçš„æœ‰æ•ˆç¨‹åº¦ï¼Œè¶Šæ˜¯ä¸€é¢å€’çš„supporting dataï¼Œè¶Šå®¹æ˜“çµ¦å‡ºconfidenceé«˜çš„çµæœã€‚ç¾åœ¨è®“æˆ‘å€‘æƒ³å›wikipediaé‚£ç¯‡æ–‡ç« ï¼Œé‚£ç¯‡æ–‡ç« çµ¦çš„æ˜¯confidenceå—ï¼Œä»–å€‘åˆ°åº•æ˜¯åœ¨rankä»€éº¼ï¼Œçœ‹èµ·ä¾†æ˜¯claimè·Ÿcitation paperçš„åŒ¹é…åº¦ï¼Œé€™æ¨£çš„è©±ä¹Ÿè¨±æˆ‘è©²æ±‚çš„ä¸æ˜¯æœ€çµ‚ç­”æ¡ˆçš„Yes, No confidenceï¼Ÿæˆ‘è©²æ±‚ä»€éº¼ï¼Ÿå¦‚æœæ˜¯æ‰¾åˆ°çš„æ–‡ç« è·Ÿå•é¡Œçš„é—œè¯ç¨‹åº¦çš„ï¼Œå°±è®Šæˆè·Ÿé€™ç¯‡wikipediaçš„æ¨¡å¼ä¸€æ¨£ï¼Œè®Šæˆæ˜¯å¤šä¸€å€‹æ–‡ç« è·Ÿcitationçš„åŒ¹é…åº¦ï¼Œç„¶å¾Œé€™å€‹åŒ¹é…åº¦å†è·Ÿå‰›å‰›ä¸Šé¢è¬›çš„æ–‡ç« å“è³ªåšä¸€å€‹åŠ æ¬Šç¸½å’Œï¼Œé€™æ¨£çš„è©±å°±ä¸éœ€è¦çµ¦å‡ºæœ€çµ‚çš„yes no confidenceäº†ï¼Œæˆ–æ˜¯å…¶å¯¦é‚„æ˜¯å¯ä»¥çµ¦ï¼Œå°±è®Šæˆå¤šä¸€å€‹åƒè€ƒä¾æ“šï¼Œé€™é‚Šè©²å¾userçš„è§€é»å»è€ƒé‡ï¼Œæˆ‘åœ¨ç”¨é€™ç³»çµ±çš„æ™‚å€™æˆ‘å€‘æ‡‰è©²æœŸæœ›é™¤äº†æœ‰Yes/Noå›ç­”å¤–ï¼Œé‚„æœ‰ä¸€å€‹é€™å€‹edgeçš„ä¿¡å¿ƒç¨‹åº¦ï¼Œæˆ–æ˜¯èªªæº–ç¢ºç¨‹åº¦ï¼Œé€™å€‹scoreè¶Šé«˜ï¼Œuserå°±è¶Šä¸ç”¨æ“”å¿ƒï¼Œè¶Šä½ï¼Œå°±ä»£è¡¨userå¯èƒ½é‚„æ˜¯å¾—è‡ªå·±å»åšæ–‡ç»å›é¡§ä¾†åˆ¤æ–·ï¼Œé‚£é€™å€‹scoreå°±ä¸æ‡‰è©²åªæ˜¯llm confidenceå› ç‚ºé€™åªåæ‡‰äº†llmå›ç­”é€™å€‹å•é¡Œçš„ä¿¡å¿ƒç¨‹åº¦ï¼Œä¸ä»£è¡¨æº–ç¢ºåº¦ï¼Œå°±ç®—æ˜¯consistencyä¹Ÿä¸ä»£è¡¨æº–ç¢ºåº¦ï¼Œé‚£éº¼æœ‰ä»€éº¼scoreæ˜¯å¯ä»¥ç•¶ä½œæº–ç¢ºåº¦çš„å‘¢ï¼Ÿç›®å‰æˆ‘é‚„æ²’æœ‰ç­”æ¡ˆï¼Œä¹Ÿè¨±è©²åšé»æ–‡ç»å›é¡§ï¼Œæˆ–æ˜¯å¯ä»¥å–®ç´”å¾æ‰¾åˆ°çš„æ–‡ç»å“è³ªï¼ŒLLM confidenceåšåˆ†æ•¸ï¼Œé‚£é€™å€‹åˆ†æ•¸è©²æ€éº¼å„ªåŒ–ï¼Œè©²æ€éº¼è®“ä»–çœŸçš„åæ‡‰accuracyã€‚ä¸å¤ªå¯èƒ½æœ‰ä»€éº¼scoreå¯ä»¥åæ‡‰accuracyï¼ŒçœŸçš„æœ‰çš„è©±å°±ä¸ç”¨benchmarkäº†ï¼Œç›´æ¥æœ€ä½³åŒ–åˆ°æœ€é«˜scoreå°±å®Œäº†ï¼Œä½†æˆ‘å€‘æ‡‰è©²é‚„æ˜¯å¯ä»¥æœ‰æŸç¨®åˆ†æ•¸å¯ä»¥æ¯”è¼ƒå¥½åæ‡‰å‡ºLLMçš„å›ç­”å“è³ªï¼Œå°±åƒæ˜¯å•ä¸€å€‹å°ˆå®¶å•é¡Œï¼Œä»–æœ‰å¯èƒ½å›ç­”å‡ºéŒ¯èª¤çš„ç­”æ¡ˆï¼Œå¯æ˜¯ä»–çš„é‚è¼¯è¡¨è¿°å¯èƒ½éƒ½æ˜¯æ­£ç¢ºçš„ï¼Œé‚£è‡³å°‘è®€è€…å¯ä»¥çŸ¥é“é€™å€‹å°ˆå®¶çš„çŒœæ¸¬æ˜¯æœ‰ä¾æ“šçš„ï¼Œä¸æ˜¯è·Ÿä½ çèªªï¼Œæ‰€ä»¥çµ¦å‡ºæº–ç¢ºçš„ç­”æ¡ˆæœ¬èº«å°±æ²’æœ‰æ„ç¾©ï¼Œæ‡‰è©²æ˜¯èªªçµ¦å‡ºæ¯”è¼ƒæœ‰ä¾æ“šçš„ç­”æ¡ˆæ‰å°ï¼Œä»¥é€™å€‹è§’åº¦æƒ³çš„è©±ï¼Œç”¨æ‰¾åˆ°çš„æ–‡ç« å“è³ªè·ŸLLM confidenceçš„ç¶œåˆåˆ†æ•¸å°±æœ‰æ„ç¾©äº†ï¼Œå†ä¾†ï¼Œæˆ‘å€‘ä¹Ÿå¯ä»¥å‡è¨­ï¼Œæ¯”è¼ƒæœ‰ä¾æ“šçš„ç­”æ¡ˆçš„æº–ç¢ºåº¦æ‡‰è©²ç›¸å°æ¯”è¼ƒé«˜æ‰å°ï¼Œæ‰€ä»¥æˆ‘å€‘å¯ä»¥é æœŸæœ€çµ‚çš„performanceæœƒæ¯”è¼ƒå¥½ï¼Œé‚£éº¼åœ¨æ‰¾åˆ°çš„æ–‡ç« éƒ½æ˜¯ä¸€æ¨£çš„æƒ…æ³ä¸‹ï¼Œæ”¹è®Šæ–‡ç« å‡ºç¾åœ¨context windowé€™ä»¶äº‹æœƒå½±éŸ¿çš„å°±åªæœ‰LLM confidenceäº†ã€‚

---
1. Agent GRN correction
    1. [Recursive Language Models](https://medium.com/@gaurav219688/deep-research-ai-workflow-using-langgraph-tavily-search-any-llm-provider-373ae5aa2cfd)
    1. Gmini 3
        1. ç”¨æˆå¤§vpnè²·ï¼Œç„¶å¾Œä¹Ÿå¯ä»¥ç›´æ¥ç”¨å°å¹£ä»˜
    1. å†çœ‹ä¸€æ¬¡wikipediaé‚£ç¯‡æ–‡ç« ï¼Œä»–å€‘rankçš„ç©¶ç«Ÿæ˜¯ä»€éº¼ï¼Œæ˜¯confidenceå—ï¼Œé‚„æ˜¯æ˜¯å…§æ–‡è·Ÿsupporting dataçš„åŒ¹é…åº¦
    1. è¼‰PPT
    1. çœ‹ä¸€ä¸‹google driveæœ‰æ²’æœ‰å…¶ä»–äººçš„PPTå¯ä»¥ç•¶åƒè€ƒ -> ç›´æ¥åœ¨google driveæœå°‹1-on-1æ˜¯å¯ä»¥æ‰¾åˆ°ï¼Œå¯ä»¥æ‹¿ä¾†åƒè€ƒ
    1. checkä¸€ä¸‹æ™‚é•·å¤šä¹… -> 25åˆ†é˜ï¼Œå¯èƒ½é‚„è¦è€ƒæ…®è¨è«–10åˆ†é˜ï¼Œæ‰€ä»¥çœ‹èµ·ä¾†å°±æ˜¯æå€‹15åˆ°20é å§
    1. next 3 or 4 months plan
        1. ç†è«–ä¸Šå°±æ˜¯ä¸€å€‹æœˆä¸€ä»¶äº‹
    1. How paper
        1. paperåŸºæœ¬structure
    1. google research confidenceé‚£ç¯‡
    1. wikipediaé‚£ç¯‡
    1. æš«å®šKGvalidatorï¼Œä¸éé€™ç¯‡æœ‰é»å°ï¼Œå¯ä»¥å†çœ‹çœ‹å…¶ä»–çš„
    1. ç¿»ç¿»LLM2KGçš„folder
    1. [KGValidator: A Framework for Automatic Validation of Knowledge Graph Construction](https://arxiv.org/abs/2404.15923)
    1. æº–å‚™å¯ä»¥è·‘å¤§è¦æ¨¡æ¨¡æ“¬çš„code
        1. checkç‚ºä»€éº¼æœ‰äº›å•é¡Œæœƒllméƒ½èªªæœ‰å•é¡Œ
            1. check true edge
                1. BCL2L1 - BAD
                1. BMI1 - H2AX
            1. false edgesè¡¨ç¾æ¯”è¼ƒå¥½çš„ç†ç”±å¯èƒ½æ˜¯å› ç‚ºæˆ‘çš„edgeå•æ³•å¤ªstrictäº†å—ï¼Œæ‰€ä»¥æ¯”è¼ƒå®¹æ˜“å›ç­”false
            1. æ¯”è¼ƒä¸€ä¸‹Auçš„ground truthè¡¨
                1. PRKACA - AKT2
                    1. å¦‚æœæˆ‘çµ¦æ­£ç¢ºçš„referenceçµ¦claudeï¼Œç„¶å¾Œå•ä»–ä¸€æ¨£çš„å•é¡Œï¼Œä»–æœƒå›ç­”ä»€éº¼
            1. ç‚ºä»€éº¼GLM-4.6åœ¨false edgeçš„è¡¨ç¾æ¯”è¼ƒå·®ï¼Ÿ
                1. çœ‹ALOX5 - STAT5A -> çœ‹èµ·ä¾†æ˜¯å°é€£æ¥çš„å®šç¾©ä¸åŒï¼Œæœ‰äº›æ˜¯çœ‹ç›´æ¥é€£æ¥ï¼Œæœ‰äº›æ˜¯é–“æ¥å°±å¯ä»¥
                    1. ç”¨ç”¨çœ‹claudeçš„searchï¼Œçœ‹ä»–æœƒciteå“ªç¯‡ï¼Œçµ¦å‡ºä»€éº¼ç­”æ¡ˆï¼Œé€™å€‹edgeæ˜¯signoråœ¨2020å¹´åŠ å…¥çš„edgeï¼Œå¾Œä¾†åˆªæ‰äº†-> çµæœsignoråè€Œæ˜¯ç”¨2007å¹´çš„paper
            1. checkå…¨éƒ¨éƒ½å›ç­”trueçš„edgeï¼Œç„¶å¾Œå•claude web search
                1. BCL2L1 - BAD -> Signorç”¨çš„æ˜¯2000å¹´çš„paper
    1. ç…§è‘—google researché‚£ç¯‡æ”¹é¡è‰²
1. Learning
    1. ç ”ç©¶abacus
    1. ç ”ç©¶gemini
    1. [Lost in the Maze: Overcoming Context Limitations in Long-Horizon Agentic Search](https://arxiv.org/abs/2510.18939)

## 251117 ##
1. [Confidence Improves Self-Consistency in LLMs](https://aclanthology.org/2025.findings-acl.1030.pdf)
1. Impemented the method True(P) above,
    1. Tested it using the qwen 14b model. Access the logprob of the first token.
    1. I tested it using the previous reasoning for with and without search. It seems the results with search often get higher confidence.
1. The plots in my hackmd are dead. I thus regenerated the plots and uploaded them again to hackmd and google drive.
1. check llm-polygraph.
1. Check token usage with gpt-oss-120b. no search 300 to 500 and with search 600 to 800.
1. Start running the H200 gpus
    1. I followed both F and Paul's tutorials. Somehow I couldn't load any model to A100 with F's tutorial. It could also because I was using interactive page.
    1. Test if I can load two llm in one H200 GPU. I can do that with these setting,
        1. `CUDA_VISIBLE_DEVICES=0 uv run python -m sglang.launch_server \
  --model-path /hps/nobackup/saezrodriguez/hf_models/gpt-oss-120b \
  --tp-size 1 \
  --tool-call-parser gpt-oss \
  --reasoning-parser gpt-oss \
  --mem-fraction-static 0.7 \
  --served-model-name gpt-oss \
  --host 0.0.0.0 \
  --port 8000`
        1. `CUDA_VISIBLE_DEVICES=1 uv run python -m sglang.launch_server \
  --model-path /hps/nobackup/saezrodriguez/hf_models/qwen3-4b-instruct-2507 \
  --tp-size 1 \
  --mem-fraction-static 0.2 \
  --served-model-name qwen2.5-14b-instruct \
  --host 0.0.0.0 \
  --port 8080`
    1. Learned how to download model. This requires export the cache path.
    1. Implemented the logprob in SGLang.
    1. I am not sure why I don't have any reasoning text showing in the result now. I need to check more latter.
1. Debbuged logprob with SGLang. It turns out the problem is the temperature. The vLLM and SGLang's logprobs are affecting by the temperature a lot while llama.cpp and openAI api are not. I now setting the temperature = 1.
1. Coding,
    1. Run single question testing.
    1. Ask Claude to help me seperate the original agent code to multiple py files so that I can also do simulation on each py file.
    1. Test running gpt-oss-120b,
        1. è·‘2è¼ªtrue+false edges -> ~23minï¼Œé‚£å°±æ˜¯ä¸€çµ„å·®ä¸å¤š10åˆ†é˜
1. Generated the benchmark result
    1. Load the prediction results and plot the AUC ROC curve. Also run gpt-oss-20b. I compared the three curves and also the heatmaps.
    1. Plot the heatmap results for all the models.
1. Made a graphic abstract. https://drive.google.com/file/d/1XZNDbX757gzJedzSXaij7MmnVbwHviwG/view?usp=sharing
1. Cite the ACl google research article.

## 251110 ##

1. Dealt with the llm confidence problem.
    1. Test asking it to think about the both side, then check the final answer's logprob. It didn't really change.
    1. I figured out that the problem is that I am using the reasoning model. With the "hidden" reasoning step, the final answer tends to be really confidence with it's own answe.
        1. Test using non-reasoning model. The resulting probability is much lower, around 0.6.
        1. [Measuring Confidence in LLM responses](https://medium.com/@georgekar91/measuring-confidence-in-llm-responses-e7df525c283f)
        1. [Is there a way to determine a LLM's confidence in an answer?](https://www.reddit.com/r/LocalLLaMA/comments/1bw87q3/is_there_a_way_to_determine_a_llms_confidence_in/)
        1. [Improving data quality with confidence](https://www.refuel.ai/blog-posts/labeling-with-confidence)
        1. [Trustworthy Language Model (TLM) - Quickstart](https://help.cleanlab.ai/tlm/tutorials/tlm/#how-does-the-tlm-trustworthiness-score-work) -> çœ‹ä¾†æ˜¯ç”¨å¦ä¸€å€‹llmä¾†è©•åˆ†ï¼Œé€™å€‹æ„Ÿè¦ºæ˜¯å¯ä»¥ç”¨åœ¨æˆ‘åœ¨æ±‚relavancyçš„æ™‚å€™
1. [Language models cannot reliably distinguish belief from knowledge and fact](https://www.nature.com/articles/s42256-025-01113-8)
1. True edges
    1. Updated SIGNOR data to Oct 2025.
    1. Sorted the True edges by Omnipath refs.
        1. Used the Omnipath api.
        1. Debugged edges without refs. I am now first using pubmed id from SIGNOR then use it to find the Omnipath ref. Then I double check the ones with no refs and use the source and target and interaction to find ref num.
    1. Filtered out the bidirectional edges.
    1. Filtered out edges with the same source node.
    1. Filtered out edges with the same target node.
    1. Write a script to test the difficulty of these edges. I test each candidate True edges with gpt-oss-20b EdgeMaster 5 times. If the EdgeMaster answer Yes for all the tests, then it means this edge is too easy thus we can remove the edge.
    1. Found [SIGNOR curation manual](https://view.officeapps.live.com/op/view.aspx?src=https%3A%2F%2Fsignor.uniroma2.it%2Fdocumentation%2FSIGNOR_curation_manual_July_2021.docx&wdOrigin=BROWSELINK)
    1. Wrote the description of the ground truth dataset preparation in the paper.

## 251103 ##

1. Start using gpt-oss-120b.
    1. Set up the llama.cpp model path.
    1. Changed the relavancy score to binary decision.
    1. Implemented ICL.
    1. Removed logits_bias.
    1. Read [OpenAI Harmony Response Format](https://cookbook.openai.com/articles/openai-harmony)
1. Tested both Travily and Searxng.
    1. Read [Best Practices for Search](https://docs.tavily.com/documentation/best-practices/best-practices-search)
    1. Add the Traviliy searching to my agent.
1. Run with vs without searching.
    1. Calculated the overall accuracy.
1. I was testing why the answering logprobs are all 0. It turns out, it was prompt problem. Instead of giving it so much constraint and wrong max_token setting. I remove all the redundent part and only ask it to return a single answer. It is not working quite good, probablly because I am using a bigger model and better formating prompt. It is now more following the prompt. Here I also list the things I have tried which did not work.
    1. Change reasoning.
    1. Asked it to think about both Yes/No possibility. "You need to think about both side of the Yes/No possibilities."
    1. Changed temperature. This one has a minor effect. It only worked after temerature > 25.
    1. I also tested to use instead of the exact logprob of the answer, I check the difference between the logprob of the answer and its' opposite. In the end, they are pure noise. I cannot get representitve confidence out of it.
1. Visualized the sentence logprobs.
1. Rewrote the prompt. Remove all the unnecessary (harmony prompt format) parts and do the formating. I also set the max_tokens to 51200, somehow it gives better result than -1. It's not that repeating anymore.
1. Debugged the token identification code.
1. Positive edge
    1. Focus on protein-protein interaction.
    1. Picked the nodes from the negative edges and used them as candidate nodes.
    1. Selected the positive edges which includes the negative edges' nodes.
1. [A Definition of AGI](https://arxiv.org/abs/2510.18212)
1. CovSyn
    1. Updated the cover letter. Make sure all our published papers are included and follows the chronologically in reverse order.
    1. Also point out that we think use of synthetic data represent a paradigm shift since only it allows us to know all quantities of interest for all individuals.
    1. Looked for the candidate editor.
    1. Solved conflicts.
    1. Submitted the article.


## 251020 ##

1. Build a simple agent. https://hackmd.io/0bNGGWGXTsSCPJk4IAWB9w?edit
    1. Debugged screen. Screen does not stop after node session closed. Using tmux can perfectly avoid the problem.
    1. Used confidence score as a loop condition. Implemented it under LangGraph structure.
        1. Test llama.cpp + langgraph.
        1. I tested if I can do llama.cpp strcutured output + logprobs. It turns out structured output does not support logprobs. Logprobs only works on pure text syntax.
        1. I also tried using two llm response approach. I first asked llm to think about the question and save the response in reasoning. Then I input the reasoning to llm and ask it to answer the question with only True or False. I also tried the ICL approach but the llm just can't generate single True/False answer so I decide to abandon this approach.
        1. Then I decided to use only ask llm once and then get the answering text token and get its' logprobs.
            1. Find the answer sentence.        
            1. Find the answer token in the anser sentence.
        1. Debugged llm repeating. I add prompt "First provide your reasoning, then on a new line write "Answer: Yes" or "Answer: No"".
    1. I was thinking using only confidence score does't make it as an agent. Because there is no llm participant in controlling the flow.
    1. I thus decide to use another llm to judge the reasoning to see if it really answering the question or it just generate some random text.
    1. Implemented the reflect_node for doing it. Now I set the confidence score 0.9 and relevance 0.7 as threshold.
    1. Generate a case that generate uncertain answer.
    1. Check if the agent can generate logprob, yes: 0.5 and No: 0.5 answer.
    1. Check Tavilysearch vs Searxng. The Searxng is free. For Tavily: 
        - 1,000 credits/month - Free. 
        - 4,000 credits/month - $30 ($0.0075/credit). 
        - Pay-as-you-go: $0.008/credit.
        - Basic search: 1 credit per request
        - Advanced search: 2 credits per request
1. Start writting Neurips paper.
    1. Downloaded the latex template.
    1. Wrote the first abstract.
1. Read [Understanding LLM Logprobs](https://medium.com/thinking-sand/understanding-llm-logprobs-029794105903)
1. Read [The Definitive Guide to LLM Temperatures](https://medium.com/thinking-sand/the-definitive-guide-to-llm-temperatures-abab311260a6)

## 251013 ##

1. Agent GRN
    1. Read [Improving Wikipedia verifiability with AI](https://www.nature.com/articles/s42256-023-00726-1)
        1. Check https://github.com/facebookresearch/side/blob/6dc4ee909655872dc0c822d9cdd0fd62fffa21d4/projects/verify_wikipedia/dpr/models/ranker_loss.py#L141
        1. Read [Document Ranking with a Pretrained Sequence-to-Sequence Model](https://arxiv.org/pdf/2003.06713). "the model is fine-tuned to produce the words "true" or "false" depending on whether the document is relevant or not to the query. At inference time, to compute probabilities for each query-document pair (in a reranking setting), we apply a softmax only on the logits of the "true" and "false" tokens. Hence, we rerank the documents according to the probabilities assigned to the "true" token. We arrived at this particular approach after some trial and error."
        1. Check [Using logprobs](https://cookbook.openai.com/examples/using_logprobs).
        1. Ollma doesn't suppport logprobs. https://github.com/ollama/ollama/issues/2415
        1. Check [optillm](https://github.com/codelion/optillm).
        1. llama.cpp
            1. Installed.
            1. Note is here https://hackmd.io/KreD_rv3RMCUYbXaLn1fmQ
    1. Web search
        1. add after:2018 in the search query.
        1. Dealt with the ddos problem.
            1. Applied random time delay.
            1. pmc is constantly not accessible. I thus exclude the pmc search results.
    1. Noted the useful github repos in my hackmd https://hackmd.io/i3-sLVu9Q7un7whI6VwfmQ.
1. CovSyn
    1. Changed the template following the Infectious Disease Modelling template.
        1. Main text.
        1. Read the author guide and added the missing sections.
            1. Author contributions.
        1. Changed to citet and citep.
        1. Proof reading Abstract and Intro.

## 250929 ##

1. Agent GRN
    1. I first test few questions with and without the web search.
        1. Does F2RL3 protein and GNAZ protein have up-regulate activity? -> ä¸ç®¡æˆ‘ç›´æ¥æ‹¿sentenceé‚„æ˜¯åŸå§‹paperå»å•claudeï¼Œå®ƒéƒ½å›ç­”æ²’æœ‰ç›´æ¥é€£æ¥ -> Does F2RL3 binds to GNAZ?é€™æ¨£å°±å¯ä»¥ï¼Œç”¨interactä¹Ÿå¯ä»¥ -> Does F2RL3 up-regulate GNAZ?é€™æ¨£ä¹Ÿå¯ä»¥ï¼Œä¹Ÿè¨±å°±åªæ˜¯å•é¡Œä¸èƒ½é€™éº¼ä»”ç´°
        1. Does MAPK3 protein upregulate SMAD4 protein? -> å¥½åƒç›´æ¥å•upregulateéƒ½æœƒæœ‰å•é¡Œï¼Œä¹Ÿè¨±æˆ‘éœ€è¦æ”¹å•æ³• -> Does MAPK3 protein enhance SMAD4 protein activity?é‚„æ˜¯false
        1. é€™å€‹caseæˆ‘å¯«Does SOD1 bind to BCL-2?ï¼Œå°±æ˜¯Trueäº† -> claude: true, gpt-oss:20b with search: true, gpt-oss:20b: False
        1. æˆ‘ç”¨Claudeæ¸¬è©¦"Does PKN1 up-regulates RAF1 phosphorylation? Return True or False."è·Ÿ"Does PKN1 up-regulates RAF1?"ï¼Œæ­£ç¢ºç­”æ¡ˆæ˜¯Trueï¼Œçµæœæ²’æœ‰è¬›phosphorylationåè€Œæ˜¯èªªå°çš„ç­”æ¡ˆ
    1. Check Perplexica, Jina, and searxng
        1. Double check the difference between the ollma-web-search with these three.
        1. Read Jina reader documentation.
            1. https://github.com/GaryKu0/ollama-web-search, it also has the image reading function.
            1. https://github.com/jina-ai/reader
            1. I also find the local Jina reader implementation, https://github.com/intergalacticalvariable/reader. I might use this if I end up reach the api limit.
        1. Check CoexistAI.
    1. My searxng step is in here https://hackmd.io/@-jBg4U8YQJi3FHeTXfRhyA/SywzmDKiee.
    1. Implemented the [ollama-web-search](https://github.com/GaryKu0/ollama-web-search) to deal with our problem.
        1. Read signore edge. Defined the questions.
        1. Run 40 repeats for the 96 negative interaction edges. I did that with and without search.
    1. Compare the restuls with and without search. I do it with a heatmap. https://hackmd.io/dlaL8abrRC2PIYwdNBl9EQ?edit
    1. Sometime it has the api problem, it seems to be the pmi problem.
    1. è®€[Singularityå®è·µæ•™ç¨‹ + Docker è½¬ Singularity çš„é¿å‘æŒ‡å—](https://blog.csdn.net/Tanqy1997/article/details/125304273)
    1. Read [Ollama Web search blog](https://ollama.com/blog/web-search) and [Ollama Web search](https://docs.ollama.com/web-search).

## 250922 ##

1. Agent GRN
    1. check [Upgrade Your AI Using Web Search - The Ollama Course](https://www.youtube.com/watch?v=GMlSFIp1na0&ab_channel=MattWilliams)
    1. Check [ollama-web-search](https://github.com/GaryKu0/ollama-web-search)
        1. Check their README.
        1. Tried using their api. I tried the serper api. The web is accessible, but I would need to change a lot on the code to make it works for searching. So I didn't complete it.
    1. check [CoexistAI](https://github.com/SPThole/CoexistAI)
    1. Tried installing docker in the cluster. It doesn't work since I don't have sudoer. Instead, I should use singularity.
    1. Check [Singularity #1 Introduction to Singularity and its difference with Docker](https://www.youtube.com/watch?v=_KhbwXqk0Bk)
    1. Check https://docs.searxng.org/admin/installation-docker.html#installation-docker.
    1. Chek https://www.reddit.com/r/LocalLLaMA/comments/1n9sod7/what_is_the_most_effective_way_to_have_your_local/. Some options are Perplexica (searxng wrapper with summary function), searxng (basic), open-webui (gui), coexistAI (lots of different functions).
    1. Implemented the ollama-wev-search.
    1. I tried CREB1 -> PCK1 and "Does TP53 protein downregulate BCL2L1?" with gpt-oss:20. I think I see some differences. The llm with search gives better result.
    1. Check [Group Relative Policy Optimization(GRPO) Visualized](https://www.youtube.com/watch?v=EX8-ucKOBbA&ab_channel=AGILambda)
1. COVID
    1. Add discussion of number of parameters in the model, number determined from prior literature, number identified based on fitting to data, number of data points used for fitting, number of parameters at bounds.
    1. Prepared the template and add the first page.

## 250916 ##

1. Agent GRN
    1. I updated the prompt and redo the llm edge denoising.
    1. Setup laptop ipynb and autocomplete.
    1. Look for signor negative edges.
        1. Downloaded each years' data.
    1. I first tried to use the source and target name to identified the removed edges, then I realize they have provide the id for each interaction.
    1. Used the signor ID to find the removed edges each year.
    1. Then I double check the source and target names of the removed edges which were identified by the ID, then I see some duplicate so I have removed those redundent edges.
    1. Check the complex BAK/BAX in the omniPath. I couldn't find it.
    1. Some of the nodes were completely missing in the 2025 graph. Since I want to find the minimum set of the removed edges, I decided to not considered the removed edges that contains the missing nodes.
    1. Why were there so many edges removed during 2021~2022. It seems there were a lot of ribosomal subunit related edges removed.
    1. I checked is there any removed edges added back to the 2025 list. I found only 1 which is {'old_signor_id': 'SIGNOR-170134', 'source_entity': 'TRPM7', 'target_entity': 'EEF2K', 'new_signor_id': 'SIGNOR-277923'}
    1. As the result, I identified 97 negative edges in signor dataset.
    1. Read [BioML-bench: Evaluation of AI Agents for End-to-End Biomedical ML](https://www.biorxiv.org/content/10.1101/2025.09.01.673319v2)
1. Check what is pyproject.toml and  __init__.py.
1. Check network topology and connectivity. I was focusing on the corrected_PKN_with_BRAF_fixes network.
    1. Check modularity and assortativity.
    1. Use Louvain algorithm to identified communities with maximizing the modularity. The modularity of the resulting communities is 0.4.
    1. Then I calculated the density for each of the communities.
    1. I also tried densest_subgraph function. But this one only works for the undirected graph.
    1. The assortativity is 0.9.
1. Check [Predicting transcriptional outcomes of novel multigene perturbations with GEARS](https://www.nature.com/articles/s41587-023-01905-6)
1. CovSyn
    1. Setup mac Latex.
    1. Presented CovSyn as a first step in a framework for benchmarking of epidemiological models and the current model as a first example of a highly specialised model demonstrating synthesis of an extreme case of COVID-19, i.e. Taiwan's first outbreak. 
    1. Stated that the aim of this framework is to enable systematic exploration of the linkage between data, modelling, analytics, and policy.
    1. Added the mata analysis article in our cover letter.
    1. Changed title to CovSynTW20: an agent-based model for synthesizing Taiwanese COVID-19 2020 course of disease and contact tracing data.
    1. Updated to say that it is a model generating synthetic data specifically for mimicing Taiwanese COVID-19 2020 outbreak
    1. Added description of Taiwanese contact tracing and isolation to motivate such a low R0.
    1. Clearly stated that this model is intended to represent this special case and not to generalise.
    1. Added the obvious example: The synthetic data can be used to compare models and show how closely they can forecast the synthetic data.
    1. Downloaded the Infectious Disease Modelling latex template.


## 250908 ##

1. Agent GRN correction
    1. Tested adding the ICL example invalid edges to the original network. It turns out the LLM just keeping throwing the ICL example as the answer. I also found out that the previous result was having similar promblem. That's probablly the reason why adding ICL example always resulting worse performance.
    1. I tried to let the examples and the PKN has the same structure and I add a prompt to ask llm to not reply any edges from the example list. Bus it does not help.
    1. Updated the random adding edges code. Now it won't add edges that cause the steps from the source node being greater than the original setting.
    1. Updated the prompt so that llm does not response the answer from the example list. Even through the llm still sometime did that. Moreover, after I have done such adjustment, llm tends to response more unstructure output.
    1. I was testing using general prompt, i.e. not specify that the llm is a molecular biologist expert. I basically just ask it to correct a graph based on its' own knowledge.
    1. I saw that the PKN has a link from MAPK1 to BRAF. I was thinking it might be an error since we also have BRAF to MAPK1. After I check the omniPath, I realized that both edges exist. But I guess the majority is BRAF -> MAPK1 since it has more refs.
    1. I also tested using another llm such as qwen3:30b and ollama. They did not perform well eigher.
    1. I think latter after I figured out a better prompt, I can add an argument to test ICL, COT, and other prompt technique.
    1. Read [Biomni-R0: Using RL to Hill-Climb Biomedical Reasoning Agents to Expert-Level](https://biomni.stanford.edu/blog/biomni-r0-technical-report/)
    1. Read [rbio1 - training scientific reasoning LLMs with biological world models as soft verifiers](https://www.biorxiv.org/content/10.1101/2025.08.18.670981v3)
    1. Check [ğŸ¦œğŸ”— LangGraph](https://art.openpipe.ai/integrations/langgraph-integration)
    1. Check [langgraph-codeact](https://github.com/langchain-ai/langgraph-codeact)
    1. Read [LLM4GRN: DISCOVERING CAUSAL GENE REGULATORY NETWORKS WITH LLMS â€“ EVALUATION THROUGH SYNTHETIC DATA GENERATION](https://arxiv.org/pdf/2410.15828)
    1. Read [Fuselinker: Leveraging Llm's Pre-Trained Text Embeddings and Domain Knowledge to Enhance Gnn-Based Link Prediction on Biomedical Knowledge Graphs](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4931757)
    1. Read [REBEL: Relation Extraction By End-to-end Language generation](https://aclanthology.org/2021.findings-emnlp.204/)
    1. Registered the nvidia workshop.

## 250902 ##

1. ICL test
    1. Read ICL
    1. Changed my prompt by providing valid and invalid examples with 10 edges for each.
        1. Focus on 3 layer network.
        1. Wrote a code to skip the result when there is no prediction at all because it generates a lot f1 0 cases.
    1. Checked how can I make the llm output follow the same structure. Eventually I used pydatic so that the llm generate json style output.
        1. Read https://llama.developer.meta.com/docs/features/structured-output
        1. Read https://ollama.ac.cn/blog/structured-outputs
        1. Read https://blog.cassdev.com/posts/llm-param-response-format/
        1. Watched https://www.youtube.com/watch?ab_channel=pixegami&t=131s&v=XIdQ6gO3Anc
        1. Debugged the code.
    1. Repeated the llm responsing for 100 times for each scenario. max_layer 3, repeat 100, add_nums 1, 2, 4, 8, 16, ICL_sizes 0, 10è·‘äº†345m52.4s -> ~5.8h. It turns out having example giving worst result than the original one.
    1. Added the explaination on the definition of valid and invalid edges. It also not helped.
1. Implemented mean reciprocal rank
    1. Read MRR.
    1. Corrected the prompt so that it generate the prediction in order. I ased the llm to predicted 16 edges.
    1. Calculated the MRR and saved them in all the results.
1. Setup zenodo.
1. Reset ollama paths
1. Read
    1. Benchmarking of signaling networks generated by large language models
    1. Extraction of molecular interaction networks with large language models
    1. [æ·±å…¥ç†è§£Reasoning LLMs](https://zhuanlan.zhihu.com/p/24869009949)
1. BRFSS
    1. Debugged Roven's code.
    1. Read https://stefvanbuuren.name/fimd/sec-pmm.html

    

## 250825 ##

1. Agent GRN
    1. Created a repository.
    1. Uploaded all my test code in the repository.
    1. Updated the Moon score plot title.
    1. Clean code
        1. Corrected ptah and cleaned code.
        1. Wrote the ignore file.
        1. Corrected R code.
        1. Put the results in Slack.
    1. Checked how many edges with 2 layers and 3 layers.
    1. Reading papers: Extraction of molecular interaction networks with large language models.
    1. Read LLM-Assisted Functional Gene Annotation.
    1. Learn MCP
        1. Watched https://www.youtube.com/watch?v=jGg_1h0qzaM&ab_channel=freeCodeCamp.org
        1. Set my mac mini conda environment.
    1. Test random added edges then ask llm to remove it.
        1. Start with the Biomni corrected network.
        1. Extracted the BRAF 2 layer network.
        1. Randomly add edges
        1. Implemented code for converting edge list to networkx to text llm input.
        1. Designed the new prompt.
        1. Check if the Biomni corrected network has cycle. The anser is yes.   
        1. Calculated the recovery precision, recall, and f1 score.
        1. I have done 30 times repeats and save the result.
        1. Plot the violin plot for the metrics.
1. Took the EBI picture.
1. BRFSS
    1. Added the excercise duration each time.
    1. Debugged the dummy encoding code.
    1. Changed the diabetes encoding way.
    1. Implemented the multiple imputation.
1. Review the causal mask in decoder.

## 250818 ##

1. I have finally settle down most of the thing in EBI.
1. Agent model GRN
    1. Setup the HPC codon cluster connection.
    1. Setup my Ollam enviroment on the HPC cluster.
    1. Setup the miniconda enviroment on the HPC cluster.
    1. Test llm's ability to reconstruct the GRN.
    1. I was tryingt to remove some of the edges from the original PKN and ask llm to reconstruct the missing edges.
        1. Design my prompt.
        1. Wrote the code to from pd table to text input for llm and another code to transform text output to llm.
        1. Implemented the basic reconstruction rate.
    1. I tried to use qwen3:8b with size 5.2 GB with the context window.
    1. Double check my reconstruction code.
    1. I tried to remove 6,761 edges and ask llm to reconstruct it. Basically none of the llm can reconstruct anything. Even biomni can only reconstruct 1 edge.
    1. I also tried to remove only 33 edges.
    1. Setup my R enviroment in the mac.
        1. Reinstall COSMOS. I did the github PAT setting. I have to install that from github repo so that I can access the moon function.
        1. Test the run_moon.R.
        1. Check the biomni logs.
1. I test if llm correction of GRN can help the final moon score.
    1. Run the moon score code with the initial PKN and also with the biomni PKN.
    1. I start with asking llm to correct the initial PKN by removing ONE edge it thinks shouldn't be there. I have tested 30 repetition and one of them shows a decreasing trend in the resulting MOON score, which is what we want. The one it removed is [REMOVE, MAP2K2, PPARG, 1]
    1. Note that Biomni removed 9 edges, which are BRAF â†’ MAP4K1, MAP2K1 â†’ TAL1, MAP2K1 â†’ IRS1, MAP2K1 â†’ GSK3B, MAP2K1 â†’ CASP9, MAP2K1 â†’ ARRB2, MAP2K1 â†’ RPS6KA4, MAP2K2 â†’ CASP9, MAP2K2 â†’ PPARG.
    1. NOTE: I commented out all whe -which code in R since the edge removal should be done by the llm agent.
    1. I have wrote a code to help me get the downstream network with defined layer number. Here I use two layers only since I saw biomni was only focusing on the second layer and part of the third layer.
    1. Designed a new prompt for this task. NOTE: the reason I stop doing the random edge removal and edge reconstruction is that I think in the end we are evaluating the llm ability by MOON score. I am not so sure what is the purpose of testing llm's ability to reconstruct a network. If I latter find a good story to connect the moon score and the reconstruction ability, I will keep testing it.
    1. Saved all the results and plot the related MOON score.
    1. I tested also asking llm to correct the GRN by removing 2, 4, and 8 edges. I check their resulting MOON score. The 8 edges removed are for example:
        1. [REMOVE, MAP2K2, PPARG, 1], [REMOVE, MAP2K2, CASP9, -1], [REMOVE, MAP4K1, CARD11, 1], [REMOVE, MAP4K1, LCP2, 1],[REMOVE, MAP2K1, TAL1, -1], [REMOVE, MAP2K1, IRS1, -1], [REMOVE, MAP2K1, CASP9, -1], [REMOVE, MAP4K1, MAP3K1, 1]
        1. [REMOVE, MAP2K2, CASP9, -1], [REMOVE, MAP2K2, PPARG, 1], [REMOVE, MAP4K1, CARD11, 1], [REMOVE, MAP4K1, LCP2, 1], [REMOVE, MAP2K1, TAL1, -1], [REMOVE, MAP2K1, IRS1, -1], [REMOVE, MAP2K1, GSK3B, 1], [REMOVE, MAP2K1, CASP9, -1]
    1. Reset my enviroment, changed the ollama model path to nfs. Also reset the other python and ollama dependencies.
1. Scholarship
    1. Read Biomni
    1. Completed the on-board
1. Connected to slack
1. Completed the renting.
1. Transfered money to wise then transfer it to my monzo.
1. Checked the ebi on-boarding sheet.
1. Job hunting
    1. Checked Taiwan Nvidia.
    1. Googled
        1. åƒé‡Œé¦¬å¾Œæ‰¾å·¥ä½œ
        1. åšå£«æ‰¾å·¥ä½œ
        1. æ‰¾å·¥ä½œ
1. BRFSS
    1. Debugged age told had cancer.
    1. Debugged variable index.
    1. Implemented the dummy encoding including health care, obese, and exercise.
    1. Read https://medium.com/we-talk-data/one-hot-encoding-vs-dummy-encoding-08d5f2665ca1
    1. Read https://ujangriswanto08.medium.com/step-by-step-guide-to-multivariate-logistic-regression-in-python-outline-7d08a416134a
    1. Meeting with Roven.
1. Studying
    1. Read Trajanoska2023
1. CovSyn
    1. Our paper was be claimed to have munipulating submission process problem. But it tured out it was their problem that they have flagged the wrong paper.

## 250718 ##

1. Read the blog posts about postdoc and industry.
1. Wrote the 104 CV first version.
    1. Updated my CV.
    1. Used the 104 AI tool to fill in the CV.
1. Used Claude to give me some comments on my CV. I saved the comments as a md file called cv_improvement_plan.md.
1. Put the Latex converter to my github.
    1. Corrected the code based on prof's comments.
    1. Created the github repository.
1. BRFSS
    1. Created a new enviroment with uv for this project.
    1. Read https://www.cdc.gov/brfss/annual_data/annual_2023.html.
    1. Check variables.
    1. Check what multivariate analysis I should use.
    1. Load all the necessary variables.
    1. Extract the data of breast cancer.
    1. Implemented the Multivariable Logistic Regression.
1. Scholarship
    1. Filled in the renting application form.
    1. Asked the renting supporting documents from the HR.
    1. Read the HR newcomer letter.
1. Graph NN
    1. Review on what is SEAL.
    1. Review on GAE and VGAE.

## 241231 ##

1. åƒåŠ 2025 åƒé‡Œé¦¬è¡Œå‰å…±è­˜ç‡Ÿ
1. çœ‹
    1. ç°½è­‰åŠå…¥å¢ƒé ˆçŸ¥ - é§è‹±åœ‹å°åŒ—ä»£è¡¨è™• Taipei Representative Office in the U.K.
    1. ã€è‹±åœ‹ç°½è­‰ã€‘2025å…¨é¢é›»å­åŒ–ï¼eVISAç”³è«‹æ­¥é©Ÿã€ä½¿ç”¨å„ªé»ä¸€æ¬¡çœ‹ï¼ - åŠæ©‹æ•™è‚²
    1. çœ‹ç°½è­‰è±å…è²¼ç°½ (Get an exempt vignette) | è‹±åœ‹ç°½è­‰ç§»æ°‘æœå‹™ | è‹±é€”ç°½è­‰ Into Visa
1. çœ‹çœ‹EMBL-EBIå®˜ç¶²æœ‰æ²’æœ‰æåˆ°é€™æ–¹é¢çš„å•é¡Œ
1. å¯„ä¿¡éå»ebiå•èªªç°½è­‰çš„æ±è¥¿

## 2410325 ##

1. 23~30çš„short conversation, short talk, and text completion.
1. Reviewed the 300 volcabulary.

## 240318 ##

1. 31~34çš„short conversation, short talk, and text completion.

## 240311 ##

1. å¯«å¤šç›Šç¬¬äº”å›

## 240306 ##

1. å ±åå¤šç›Š
1. å¯«NCKUå…©å›è©¦é¡Œ

## 240228 ##

1. æˆ‘æ±ºå®šå ±å3/31çš„å¤šç›Šè€ƒè©¦
1. è¤‡ç¿’196å€‹å¤šç›Šå–®å­—
1. è¤‡ç¿’æ–‡æ³•
1. å¯«NCKUå…©å›ç·´ç¿’é¡Œ

## 231011 ##

1. Read https://zhuanlan.zhihu.com/p/149186533.

## 230904 ##

1. é‡æ–°ç”¢ç”Ÿcropped imageè·Ÿconcentration
1. è™•ç†torch tensorå•é¡Œ
1. è©¦data augmentation (rotation)
1. Trainä¸€å€‹tinyVGG modelï¼Œä½†æ˜¯performanceä¸ä½³
1. å®ŒæˆSection 10èª²ç¨‹

## 230828 ##

1. çœ‹å®ŒPytorch course 7~9
1. æŸ¥ç³»çµ±ç”Ÿç‰©å­¸çš„é ˜å°åœ˜éšŠè·Ÿæ›¸ç±ï¼Œé †ä¾¿çœ‹SBIä»–å€‘paperéƒ½åœ¨å¯«ç”šéº¼ï¼ŒåŠ å…¥ä»–å€‘éœ€è¦ç”šéº¼èƒ½åŠ›ã€‚

## 230821 ##

1. å®ŒæˆPytorch course 5 & 6
1. è®€è®€Making Deep Learning Go Brrrr From First Principles
1. æ•´ç†éƒµä»¶

## 230817 ##

1. I bought the Udemy open course "PyTorch for Deep Learning in 2023: Zero to Mastery".
1. I finished until section 4 and I have done the classification homework.

## 230814 ##

1. Manage to my todos.
1. å®ŒæˆPytorch course section 5.

## 230529 ##

1. Add the COMPSAC preprint to my CV and LinkedIn

## 230517 ##

1. è·‘Peteræ‰¾åˆ°çš„æ¨¡å‹çš„shæª”
1. æ¸¬è©¦data preprocessing
1. è¨‚ç¾©å¤§åˆ©è¡Œç¨‹è·Ÿroman pass
1. æº–å‚™ï¼Œç›®æ¨™å…¬å¸ï¼ŒCVï¼ŒPPTï¼ŒåƒåŠ Seminaræ¨¡æ“¬é¢è©¦

## 230510 ##

1. Updated my linkedin profile. Add most of the courses, award, and publication on it.
1. é‡å¯«linkedin about
1. ç¾©å¤§åˆ©è¨‚æˆ¿
## 230503 ##

1. æŸ¥ç±³è˜­ï¼Œå°¼æ–¯ï¼Œæ‘©ç´å“¥ï¼Œä½›ç¾…å€«æ–¯çš„è³‡æ–™

## 230403 ##

1. ä¿®æ”¹Hondurasçš„gitè¨­å®šï¼ŒæŠŠå®ƒæ”¹æˆsshé€£ç·šã€‚
1. è™•ç†Ubuntuçš„æ¿¾è—å…‰ç¨‹å¼ã€‚
1. å¯„Linkediné‚€è«‹çµ¦Seminarè¬›è€…ã€‚

## 230327 ##

1. åœ¨ubuntuä¸Šé‡è£vscode
1. æ•´ç†æˆ‘åœ¨taskadeä¸Šçš„todoï¼ŒæŠŠä»–æ”¹æˆå‹•ä½œçš„æ•˜è¿°ï¼Œä¸¦åŠ ä¸Šdeadline
1. è®€Sapiens ch1
1. è®€Systems biology simulation of dynamic network states ch1~ch6ï¼Œå¤§è‡´ä¸Šè®€å®Œï¼Œæ‡‰è©²æš«æ™‚ä¸æœƒè®€ä¸‹å»
1. å¤§è‡´æ›´æ–°Linkedinçš„Educationè³‡æ–™
1. æ¸¬è©¦git config

## 230313 ##

1. è®€Deep learning ch6
2. éLinkedin Machine learning test.
3. æŸ¥ä¸€ä¸‹Rikençš„çµ„ç¹”ä»¥åŠinternshitï¼Œçœ‹èµ·ä¾†æ‡‰è©²æ˜¯ç›´æ¥è™•ç†åƒé‡Œé¦¬è¨ˆç•«æ¯”è¼ƒå¿«

## 230303 ##

1. éLinkedin Matlab test.
2. è®€Deep learning ch3~ch5.

## 230208 ##

1. è¾¦indeedï¼Œglassdoorï¼Œ104çš„å¸³è™Ÿ
2. Checkå¹¾å‰‡linkedinä¸Šé—œæ–¼systems biologyè·Ÿbioinformaticsçš„å·¥ä½œï¼Œå¥½åƒéƒ½æ˜¯åœ¨è™•ç†NGSçš„data
3. åœ¨Colabä¸Šè¨­å®špytorchè·ŸGPUé‚„æœ‰mount google drive
4. å¯«æå¼˜æ¯…é–‹æ”¾èª²ç¨‹çš„HW1
5. Deep Learning with PyTorch ch1 and ch2

## 221228 ##

1. è¨­å®šcolabï¼Œè©¦ç”¨
2. çœ‹deep learning week 1 lectures

## 221212 ##

1. è·ŸSeminarè¬›è€…èŠå¤©(An-Chi Wei)
2. åƒåŠ GIWé¤å®´

## 221130 ##

1. å¯«å…©ç¯‡å¤šç›Šè½åŠ›

## 221107 ##

1. è™•ç†æœƒè¨ˆç›¸é—œçš„data

## 221031 ##

1. å­¸å†°æ»´
2. æ´—é‹
3. å¯„è‹±æ–‡è«®è©¢æ–‡ä»¶
4. Make a table of habit list. https://docs.google.com/spreadsheets/d/1kGhHPQfNOy0xTSpWFhkD_LoLLzfyNwHLt3S4yiWEwVs/edit?usp=share_link

## 221024 ##

1. Memorized TOEIC vocabulary.
2. Wrote one TOEIC reading.
3. Made this wiki page.
4. Managed my tabs.
5. Clean all the mails.
6. Applied the æ°¸è± scholarship.

# Welcome

Welcome to your wiki! This is the default page we've installed for your convenience. Go ahead and edit it.

## Wiki features

This wiki uses the [Markdown](http://daringfireball.net/projects/markdown/) syntax. The [MarkDownDemo tutorial](https://bitbucket.org/tutorials/markdowndemo) shows how various elements are rendered. The [Bitbucket documentation](https://confluence.atlassian.com/x/FA4zDQ) has more information about using a wiki.

The wiki itself is actually a git repository, which means you can clone it, edit it locally/offline, add images or any other file type, and push it back to us. It will be live immediately.

Go ahead and try:

```
$ git clone https://Rain_wu_Nordlinglab@bitbucket.org/Rain_wu_Nordlinglab/rain-wu-personal.git/wiki
```

Wiki pages are normal files, with the .md extension. You can edit them locally, as well as creating new ones.

## Syntax highlighting


You can also highlight snippets of text (we use the excellent [Pygments][] library).

[Pygments]: http://pygments.org/


Here's an example of some Python code:

```python

def wiki_rocks(text):
    formatter = lambda t: "funky"+t
    return formatter(text)
```


You can check out the source of this page to see how that's done, and make sure to bookmark [the vast library of Pygment lexers][lexers], we accept the 'short name' or the 'mimetype' of anything in there.
[lexers]: http://pygments.org/docs/lexers/


Have fun!